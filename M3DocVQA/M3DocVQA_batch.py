import os
import json
import ujson
import fitz  # PyMuPDF
import time
import random
import shutil
import hashlib
from dotenv import load_dotenv
from typing import List, Tuple, Dict, Any, Set, Optional
from dataclasses import dataclass
from tqdm import tqdm
from PIL import Image
import asyncio
from typing import List, Dict, Any, Generator

# LlamaIndex ê´€ë ¨ ìž„í¬íŠ¸
from llama_index.core import (
    Document,
    VectorStoreIndex,
    Settings,
    StorageContext,
    SimpleDirectoryReader,
    PropertyGraphIndex
)
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor
from llama_index.llms.openai import OpenAI
from llama_index.core.llms import CustomLLM, LLMMetadata, CompletionResponse, ChatResponse
from llama_index.core.base.llms.types import ChatMessage
from llama_index.embeddings.openai import OpenAIEmbedding
from openai import OpenAI as OpenAIClient # Batch ì—…ë¡œë“œìš©
from google import genai

# =========================================================
# [ì„¤ì • ì˜ì—­] API KEY ë° ê²½ë¡œ
# =========================================================

load_dotenv()
GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
BASE_DIR = os.environ.get('BASE_DIR') 
PDF_DIR = os.path.join(BASE_DIR, "pdfs_dev")
DEV_QA_PATH = os.path.join(BASE_DIR, "multimodalqa", "MMQA_dev.jsonl")

# --- [Batch ì„¤ì •] ---
# ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: "RECORD" (ìš”ì²­ íŒŒì¼ ìƒì„±) or "REPLAY" (ê²°ê³¼ë¡œ í‰ê°€)
# ìµœì´ˆ ì‹¤í–‰ ì‹œ "RECORD" -> Batch ì—…ë¡œë“œ/ì™„ë£Œ -> "REPLAY"ë¡œ ë³€ê²½ í›„ ì‹¤í–‰
#EXECUTION_MODE = "RECORD" 
EXECUTION_MODE = "REPLAY"

BATCH_INPUT_FILE = "batch_input.jsonl"   # ìƒì„±ë  ìš”ì²­ íŒŒì¼
BATCH_OUTPUT_FILE = "batch_output.jsonl" # ë‹¤ìš´ë¡œë“œ ë°›ì€ ê²°ê³¼ íŒŒì¼
MODEL_NAME = "gpt-4o-mini"
# ------------------------

Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
client = genai.Client(api_key=GOOGLE_API_KEY)
vision_model_id = 'gemini-2.0-flash'

TOP_K_LIST = [1, 3, 5, 10]
LIMIT_PDFS = 10

# =========================================================
# [Part 1] Batch API ì§€ì›ìš©
# =========================================================
class BatchMockLLM(CustomLLM):
    """
    OpenAI Batch APIìš© ëž˜í¼. 
    chat, complete, async í˜¸ì¶œ ë“± ëª¨ë“  ê²½ë¡œë¥¼ ê°€ë¡œì±„ì„œ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    mode: str = "RECORD"
    requests_buffer: List[Dict] = []
    responses_cache: Dict[str, str] = {}
    model_name: str = MODEL_NAME

    def __init__(self, mode: str, output_file: str = None):
        super().__init__()
        self.mode = mode
        if mode == "REPLAY" and output_file and os.path.exists(output_file):
            print(f"ðŸ“‚ Batch ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘: {output_file}")
            with open(output_file, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        cid = data.get("custom_id")
                        content = data["response"]["body"]["choices"][0]["message"]["content"]
                        self.responses_cache[cid] = content
                    except Exception as e:
                        pass
            print(f"âœ… {len(self.responses_cache)}ê°œì˜ ì‘ë‹µ ë¡œë“œ ì™„ë£Œ.")

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(model_name=self.model_name)

    def _get_hash(self, messages: List[ChatMessage]) -> str:
        """ë©”ì‹œì§€ ë‚´ìš©ìœ¼ë¡œ ê³ ìœ  ID ìƒì„±"""
        content_str = "".join([f"{m.role}:{m.content}" for m in messages])
        return hashlib.md5(content_str.encode("utf-8")).hexdigest()

    # --- [í•µì‹¬ ë¡œì§] íŒŒì¼ ê¸°ë¡ ë° ê°€ë¡œì±„ê¸° ---
    def chat(self, messages: List[ChatMessage], **kwargs) -> ChatResponse:
        custom_id = self._get_hash(messages)

        if self.mode == "RECORD":
            req = {
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": self.model_name,
                    "messages": [{"role": m.role.value, "content": m.content} for m in messages],
                    "temperature": 0
                }
            }
            # íŒŒì¼ì— ì¦‰ì‹œ ì“°ê¸°
            with open(BATCH_INPUT_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(req, ensure_ascii=False) + "\n")
            
            # ë”ë¯¸ ì‘ë‹µ (JSON íŒŒì‹± ì—ëŸ¬ ë°©ì§€ìš©)
            return ChatResponse(message=ChatMessage(role="assistant", content="[]"))

        elif self.mode == "REPLAY":
            content = self.responses_cache.get(custom_id, "[]")
            return ChatResponse(message=ChatMessage(role="assistant", content=content))

    # --- [ë¦¬ë‹¤ì´ë ‰íŠ¸] ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ ì‹œì—ë„ ë¬´ì¡°ê±´ chat ë¡œì§ì„ íƒœì›€ ---
    
    def complete(self, prompt: str, **kwargs) -> CompletionResponse:
        # complete ìš”ì²­ì´ ì˜¤ë©´ user messageë¡œ ê°ì‹¸ì„œ chatìœ¼ë¡œ ë³´ëƒ„
        msg = ChatMessage(role="user", content=prompt)
        chat_response = self.chat([msg], **kwargs)
        return CompletionResponse(text=chat_response.message.content)

    async def achat(self, messages: List[ChatMessage], **kwargs) -> ChatResponse:
        # ë¹„ë™ê¸° ìš”ì²­ë„ ë™ê¸° chat ë©”ì„œë“œë¡œ ì—°ê²° (íŒŒì¼ ì“°ê¸°ëŠ” blockingì´ì–´ë„ ë¬´ë°©)
        return self.chat(messages, **kwargs)

    async def acomplete(self, prompt: str, **kwargs) -> CompletionResponse:
        return self.complete(prompt, **kwargs)

    # --- [ì¸í„°íŽ˜ì´ìŠ¤ ì¤€ìˆ˜ìš©] ---
    def stream_chat(self, messages: List[ChatMessage], **kwargs: Any) -> Generator[ChatResponse, None, None]:
        yield self.chat(messages, **kwargs)

    def stream_complete(self, prompt: str, **kwargs: Any) -> Generator[CompletionResponse, None, None]:
        yield self.complete(prompt, **kwargs)

# =========================================================
# [Part 2] ë°ì´í„° ë¡œë” (ê¸°ì¡´ê³¼ ë™ì¼)
# =========================================================

@dataclass
class QAExample:
    qid: str
    question: str
    answers: List[str]
    positive_doc_ids: Set[str]

def load_mmqa_data(jsonl_path: str) -> List[QAExample]:
    qa_list = []
    if not os.path.exists(jsonl_path): return []

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            ex = ujson.loads(line)
            positive_doc_ids = set()
            if "supporting_context" in ex:
                for ctx in ex["supporting_context"]:
                    if ctx.get("doc_id"): positive_doc_ids.add(str(ctx.get("doc_id")))
            
            if positive_doc_ids:
                qa_list.append(QAExample(ex.get("qid"), ex["question"], 
                                       [a["answer"] if isinstance(a, dict) else a for a in ex.get("answers", [])], 
                                       positive_doc_ids))
    return qa_list

class SmartPDFLoader:
    def __init__(self, pdf_dir: str):
        self.pdf_dir = pdf_dir
    
    def load_specific_documents(self, target_doc_ids: Set[str]) -> List[Document]:
        all_docs = []
        available_files = {f.split('.')[0]: f for f in os.listdir(self.pdf_dir) if f.lower().endswith(".pdf")}
        found_files = [available_files[tid] for tid in target_doc_ids if tid in available_files]
        
        for filename in tqdm(found_files, desc="ðŸ“‚ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘", unit="file"):
            filepath = os.path.join(self.pdf_dir, filename)
            doc_id = filename.split('.')[0]
            try:
                text_docs = SimpleDirectoryReader(input_files=[filepath]).load_data()
                for d in text_docs: d.metadata["doc_id"] = doc_id
                all_docs.extend(text_docs)
            except: pass
        return all_docs

# =========================================================
# [Part 3] ì‹¤í–‰ ë° í‰ê°€
# =========================================================

def upload_batch_file():
    """RECORD ëª¨ë“œë¡œ ìƒì„±ëœ íŒŒì¼ì„ OpenAIì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(BATCH_INPUT_FILE):
        print(f"âŒ {BATCH_INPUT_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    c = OpenAIClient(api_key=OPENAI_API_KEY)
    print("\nâ˜ï¸ [OpenAI] Batch íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
    batch_file = c.files.create(file=open(BATCH_INPUT_FILE, "rb"), purpose="batch")
    
    print("ðŸš€ [OpenAI] Batch ìž‘ì—… ìƒì„± ì¤‘...")
    batch_job = c.batches.create(
        input_file_id=batch_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h"
    )
    print(f"âœ… Batch ìž‘ì—… ì‹œìž‘ë¨! ID: {batch_job.id}")
    print("â³ ìž‘ì—…ì´ ì™„ë£Œ(completed)ë˜ë©´ ê²°ê³¼ íŒŒì¼(output_file_id)ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬")
    print(f"   '{BATCH_OUTPUT_FILE}' ì´ë¦„ìœ¼ë¡œ ì €ìž¥í•œ ë’¤ EXECUTION_MODE='REPLAY'ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")

def evaluate_system():
    # 1. ì´ì „ íŒŒì¼ ì •ë¦¬ (RECORD ëª¨ë“œì¼ ë•Œë§Œ)
    if EXECUTION_MODE == "RECORD" and os.path.exists(BATCH_INPUT_FILE):
        os.remove(BATCH_INPUT_FILE)

    # [í•µì‹¬ ë³€ê²½] Settings.llm êµì²´
    Settings.llm = BatchMockLLM(mode=EXECUTION_MODE, output_file=BATCH_OUTPUT_FILE)

    print("\n" + "="*40)
    print(f"ðŸš€ MMQA ì‹œìŠ¤í…œ ì‹œìž‘ | ëª¨ë“œ: {EXECUTION_MODE}")
    print("="*40)

    # --- ë°ì´í„° ë¡œë“œ ë¶€ë¶„ ---
    full_qa_list = load_mmqa_data(DEV_QA_PATH)
    if not full_qa_list: return
    
    needed_doc_ids = set()
    for qa in full_qa_list: needed_doc_ids.update(qa.positive_doc_ids)
    available_pdfs = set(f.split('.')[0] for f in os.listdir(PDF_DIR) if f.endswith(".pdf"))
    valid_doc_ids = list(needed_doc_ids.intersection(available_pdfs))
    
    if not valid_doc_ids:
        print("âŒ ë§¤ì¹­ PDF ì—†ìŒ")
        return

    random.seed(42) # ì¼ê´€ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì •
    random.shuffle(valid_doc_ids)
    target_doc_ids = set(valid_doc_ids[:LIMIT_PDFS])
    
    loader = SmartPDFLoader(PDF_DIR)
    docs = loader.load_specific_documents(target_doc_ids)
    
    # --- ì¸ë±ì‹± (ì—¬ê¸°ì„œ LLM í˜¸ì¶œ ë°œìƒ) ---
    print(f"ðŸ—ï¸  Knowledge Graph ì²˜ë¦¬ ì¤‘... (Mode: {EXECUTION_MODE})")
    
    # RECORD ëª¨ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ íŒŒì¼ë§Œ ì“°ê³ , ì‹¤ì œ ì¸ë±ìŠ¤ëŠ” í…… ë¹ˆ ìƒíƒœê°€ ë¨
    try:
        index = PropertyGraphIndex.from_documents(
            docs,
            embed_model=Settings.embed_model,
            llm=Settings.llm,
            kg_extractors=[SimpleLLMPathExtractor(llm=Settings.llm, max_paths_per_chunk=5)],
            show_progress=True 
        )
    except Exception as e:
        # RECORD ëª¨ë“œì¼ ë•Œ ë”ë¯¸ ì‘ë‹µ ë•Œë¬¸ì— íŒŒì‹± ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìžˆìœ¼ë‚˜ ë¬´ì‹œí•´ë„ ë¨
        if EXECUTION_MODE == "RECORD": pass
        else: print(f"âš ï¸ Warning: {e}")

    # --- RECORD ëª¨ë“œ ì¢…ë£Œ ì²˜ë¦¬ ---
    if EXECUTION_MODE == "RECORD":
        print(f"\nâœ… [RECORD ì™„ë£Œ] '{BATCH_INPUT_FILE}' ìƒì„±ë¨.")
        print("   ì´ì œ OpenAIì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
        upload_batch_file()
        return

    # --- REPLAY ëª¨ë“œ í‰ê°€ ìˆ˜í–‰ ---
    retriever = index.as_retriever(include_text=True, similarity_top_k=max(TOP_K_LIST))
    filtered_qa_list = [qa for qa in full_qa_list if not qa.positive_doc_ids.isdisjoint(target_doc_ids)]
    
    print(f"ðŸ”Ž í‰ê°€ ì‹œìž‘ ({len(filtered_qa_list)} questions)")
    metrics = {f"recall@{k}": 0.0 for k in TOP_K_LIST}
    metrics.update({f"mrr@{k}": 0.0 for k in TOP_K_LIST})
    
    for i, ex in enumerate(tqdm(filtered_qa_list, desc="Evaluaton")):
        try:
            nodes = retriever.retrieve(ex.question)
        except:
            nodes = []

        retrieved_details = []
        retrieved_doc_ids = []  # ID ì €ìž¥ìš© ë¦¬ìŠ¤íŠ¸

        for node in nodes:
            r_doc_id = node.metadata.get("doc_id", "")
            retrieved_doc_ids.append(r_doc_id) # [ë²„ê·¸ìˆ˜ì •] ID ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            retrieved_details.append(f"[{r_doc_id}] {node.text[:50]}...")

        gt_set = ex.positive_doc_ids
        
        # [ë¡œê·¸ ì¶œë ¥]
        tqdm.write(f"\nðŸ“Œ Q: {ex.question}")
        tqdm.write(f"âœ… GT: {list(gt_set)}")
        # [ë²„ê·¸ìˆ˜ì •] retrieved_doc_ids ì‚¬ìš©
        hit_check = any(d in gt_set for d in retrieved_doc_ids[:5]) 
        tqdm.write(f"ðŸŽ¯ Hit: {hit_check}")

        for k in TOP_K_LIST:
            current_top_k = retrieved_doc_ids[:k]
            if any(did in gt_set for did in current_top_k):
                metrics[f"recall@{k}"] += 1.0
            for rank, did in enumerate(current_top_k, 1):
                if did in gt_set:
                    metrics[f"mrr@{k}"] += 1.0 / rank
                    break

    # ê²°ê³¼ ì¶œë ¥
    count = len(filtered_qa_list)
    print("\nðŸ“Š ê²°ê³¼")
    if count > 0:
        for k in TOP_K_LIST:
            print(f"K={k:<2} | Recall: {metrics[f'recall@{k}']/count:.4f} | MRR: {metrics[f'mrr@{k}']/count:.4f}")

if __name__ == "__main__":
    evaluate_system()