import os
import json
import requests
import time
from dotenv import load_dotenv
from datasets import load_dataset
from tqdm import tqdm

# =========================================================
# [ì„¤ì • ì˜ì—­]
# =========================================================
load_dotenv()
# ë°ì´í„°ë¥¼ ì €ì¥í•  ê¸°ë³¸ ê²½ë¡œ
BASE_DIR = os.environ.get('QASPER_DIR') 

# ì €ì¥ë  ê²½ë¡œ ì„¤ì •
QASPER_DATA_DIR = os.path.join(BASE_DIR, "qasper")
PDF_DIR = os.path.join(BASE_DIR, "qasper_pdfs")
JSON_SAVE_PATH = os.path.join(QASPER_DATA_DIR, "qasper-dev-v0.3.json")

# PDF ë‹¤ìš´ë¡œë“œ ê°œìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´ ë‹¤ìš´ë¡œë“œ)
LIMIT_PDF_DOWNLOAD = 50

# =========================================================
# [ê¸°ëŠ¥ êµ¬í˜„]
# =========================================================

def setup_directories():
    if not os.path.exists(QASPER_DATA_DIR):
        os.makedirs(QASPER_DATA_DIR)
    if not os.path.exists(PDF_DIR):
        os.makedirs(PDF_DIR)
    print(f"ğŸ“‚ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ:\n - JSON: {QASPER_DATA_DIR}\n - PDF: {PDF_DIR}")

def download_and_convert_json():
    print("â¬‡ï¸  Hugging Faceì—ì„œ QASPER ë°ì´í„°ì…‹(validation) ë‹¤ìš´ë¡œë“œ ì¤‘...")
    # 'validation' ì…‹ì„ dev ì…‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ds = load_dataset("allenai/qasper", split="validation")
    
    formatted_data = {}
    
    print("ğŸ”„ ë°ì´í„° ë³€í™˜ ì¤‘ (HF format -> Original Dictionary format)...")
    for entry in ds:
        paper_id = entry['id']
        
        # Hugging Face í¬ë§·ì„ ì´ì „ í‰ê°€ ì½”ë“œê°€ ì½ì„ ìˆ˜ ìˆëŠ” Dict í˜•íƒœë¡œ ë³€í™˜
        formatted_data[paper_id] = {
            "title": entry['title'],
            "abstract": entry['abstract'],
            "qas": []
        }
        
        # QA ë°ì´í„° êµ¬ì¡° ë³€í™˜
        # HF ë°ì´í„°ì…‹ì˜ 'qas'ëŠ” ë¦¬ìŠ¤íŠ¸ë“¤ì˜ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ìˆ˜ ìˆì–´ íŒŒì‹± í•„ìš”
        qas_raw = entry['qas']
        
        # question_idì™€ question ê°œìˆ˜ë§Œí¼ ìˆœíšŒ
        num_qas = len(qas_raw['question_id'])
        
        for i in range(num_qas):
            qa_obj = {
                "question_id": qas_raw['question_id'][i],
                "question": qas_raw['question'][i],
                "answers": []
            }
            
            # Answers ì²˜ë¦¬ (answer ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ êµ¬ì¡° ì²˜ë¦¬)
            # HF Datasetì˜ êµ¬ì¡°ê°€ ë³µì¡í•˜ë¯€ë¡œ, ë‹¨ìˆœí™”í•˜ì—¬ ì •ë‹µ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
            # (ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ êµ¬ì¡°ë§Œ ì¡ê³ , ì‹¤ì œ ë‚´ìš©ì€ ì›ë³¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ë”°ë¦„)
            
            raw_answers = qas_raw['answers'][i] # This is a dict with lists
            
            # answer_id ê°œìˆ˜ë§Œí¼ ìˆœíšŒ (í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ì—¬ëŸ¬ ë‹µë³€ìê°€ ìˆì„ ìˆ˜ ìˆìŒ)
            # HF QASPER êµ¬ì¡°: answers -> {'answer': [{'free_form_answer': ..., 'highlighted_evidence': ...}]}
            
            # ë°ì´í„°ì…‹ ë²„ì „/êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            answer_list = raw_answers.get('answer', [])
            
            converted_answers = []
            for ans in answer_list:
                converted_answers.append({"answer": ans})
                
            qa_obj["answers"] = converted_answers
            formatted_data[paper_id]["qas"].append(qa_obj)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(JSON_SAVE_PATH, 'w', encoding='utf-8') as f:
        json.dump(formatted_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSON ë°ì´í„° ì €ì¥ ì™„ë£Œ: {JSON_SAVE_PATH} (ì´ {len(formatted_data)}ê°œ ë…¼ë¬¸)")
    return list(formatted_data.keys())

def download_pdfs(paper_ids):
    print(f"\nâ¬‡ï¸  PDF ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ì´ {len(paper_ids)}ê°œ ëŒ€ìƒ)")
    
    success_count = 0
    fail_count = 0
    skip_count = 0
    
    # ì œí•œ ì„¤ì • ì ìš©
    target_ids = paper_ids[:LIMIT_PDF_DOWNLOAD] if LIMIT_PDF_DOWNLOAD else paper_ids
    
    for pid in tqdm(target_ids, desc="Downloading PDFs"):
        # arXiv IDë¡œ PDF URL ìƒì„±
        pdf_url = f"https://arxiv.org/pdf/{pid}.pdf"
        save_path = os.path.join(PDF_DIR, f"{pid}.pdf")
        
        # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
        if os.path.exists(save_path):
            skip_count += 1
            continue
            
        try:
            # arXiv ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ (í•„ìˆ˜)
            time.sleep(3) 
            
            response = requests.get(pdf_url, timeout=30)
            if response.status_code == 200:
                with open(save_path, 'wb') as f:
                    f.write(response.content)
                success_count += 1
            else:
                # print(f"Failed to download {pid}: Status {response.status_code}")
                fail_count += 1
        except Exception as e:
            # print(f"Error downloading {pid}: {e}")
            fail_count += 1
            
    print("\n" + "="*40)
    print(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ìš”ì•½")
    print(f" - ì„±ê³µ: {success_count}")
    print(f" - ì‹¤íŒ¨: {fail_count}")
    print(f" - ìŠ¤í‚µ(ì´ë¯¸ ìˆìŒ): {skip_count}")
    print(f" - ì €ì¥ ê²½ë¡œ: {PDF_DIR}")
    print("="*40)

if __name__ == "__main__":
    setup_directories()
    
    # 1. JSON ë°ì´í„° ì¤€ë¹„
    all_paper_ids = download_and_convert_json()
    
    # 2. PDF ë‹¤ìš´ë¡œë“œ
    download_pdfs(all_paper_ids)