import os
import json
import random
from typing import List, Set
from dataclasses import dataclass
from tqdm import tqdm
from dotenv import load_dotenv

# LlamaIndex Core
from llama_index.core import (
    Document,
    Settings,
    SimpleDirectoryReader,
    PropertyGraphIndex, 
    StorageContext
)
# [ë³€ê²½] ëŠë¦° LLM ì¶”ì¶œê¸° ëŒ€ì‹ , ë¹ ë¥¸ 'Implicit' ì¶”ì¶œê¸° ì‚¬ìš©
from llama_index.core.indices.property_graph import ImplicitPathExtractor
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# Retrieval & Post-processing
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.retrievers.fusion_retriever import FUSION_MODES
from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank

# =========================================================
# [1. ì„¤ì • ì˜ì—­]
# =========================================================

load_dotenv()

BASE_DIR = os.environ.get('QASPER_DIR')
PDF_DIR = os.path.join(BASE_DIR, "qasper_pdfs")
QASPER_JSON_PATH = os.path.join(BASE_DIR, "qasper", "qasper-dev-v0.3.json")

# ëª¨ë¸ ì„¤ì •
Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# ì²­í¬ ì„¤ì •
parser = SentenceSplitter(chunk_size=1024, chunk_overlap=128)
Settings.node_parser = parser

TOP_K_LIST = [1, 3, 5, 10]
LIMIT_PDFS = 30 

# =========================================================
# [2. ë°ì´í„° ë¡œë”] 
# =========================================================

@dataclass
class QAExample:
    qid: str
    question: str
    answers: List[str]
    positive_doc_ids: Set[str]

def load_qasper_data(json_path: str) -> List[QAExample]:
    if not os.path.exists(json_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return []

    print(f"ğŸ“– QASPER ë°ì´í„° íŒŒì‹± ì¤‘...")
    with open(json_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    qa_list = []
    for paper_id, content in raw_data.items():
        qas = content.get("qas", [])
        for qa in qas:
            extracted_answers = []
            for ans_obj in qa.get("answers", []):
                ans_data = ans_obj.get("answer", {})
                if ans_data.get("unanswerable", False): continue 
                
                if ans_data.get("extractive_spans"):
                    extracted_answers.extend(ans_data["extractive_spans"])
                elif ans_data.get("free_form_answer"):
                    extracted_answers.append(ans_data["free_form_answer"])
                elif ans_data.get("yes_no") is not None:
                    extracted_answers.append(str(ans_data["yes_no"]))
            
            if extracted_answers:
                qa_list.append(QAExample(
                    qid=qa.get("question_id"),
                    question=qa.get("question"),
                    answers=extracted_answers,
                    positive_doc_ids={str(paper_id)}
                ))
    return qa_list

# =========================================================
# [3. ë¬¸ì„œ ë¡œë”] ë©”íƒ€ë°ì´í„° ì£¼ì…
# =========================================================

class OptimizedPDFLoader:
    def __init__(self, pdf_dir: str):
        self.pdf_dir = pdf_dir

    def load_specific_documents(self, target_doc_ids: Set[str]) -> List[Document]:
        all_docs = []
        available_files = {f.replace('.pdf', ''): f for f in os.listdir(self.pdf_dir) if f.endswith('.pdf')}
        
        found_ids = [tid for tid in target_doc_ids if tid in available_files]
        if not found_ids: return []

        print(f"ğŸ“‚ PDF ë¡œë”© ë° ë©”íƒ€ë°ì´í„° ì£¼ì… ({len(found_ids)}ê°œ)...")
        for doc_id in tqdm(found_ids, desc="Processing PDFs"):
            filename = available_files[doc_id]
            filepath = os.path.join(self.pdf_dir, filename)
            
            try:
                text_docs = SimpleDirectoryReader(input_files=[filepath]).load_data()
                
                for d in text_docs:
                    d.metadata["doc_id"] = doc_id
                    d.metadata["file_name"] = filename
                    
                    header = f"[Paper ID: {doc_id}]\n"
                    original_text = d.get_content()
                    d.set_content(header + original_text)

                all_docs.extend(text_docs)
            except Exception as e:
                print(f"   [Error] {filename}: {e}")
                
        return all_docs

# =========================================================
# [4. ë©”ì¸ í‰ê°€ ë¡œì§] FAST PropertyGraph + Hybrid + Rerank
# =========================================================

def evaluate_system():
    print("\n" + "="*70)
    print("ğŸš€ QASPER Fast Graph Evaluation")
    print("   1. Metadata Injection")
    print("   2. Fast Property Graph (Implicit Structure)")
    print("   3. Hybrid Search + Re-ranking")
    print("="*70)

    # 1. ë°ì´í„° ì¤€ë¹„
    full_qa_list = load_qasper_data(QASPER_JSON_PATH)
    if not full_qa_list or not os.path.exists(PDF_DIR): return

    available_pdfs_ids = set(f.replace('.pdf', '') for f in os.listdir(PDF_DIR) if f.endswith(".pdf"))
    needed_ids = set()
    for qa in full_qa_list: needed_ids.update(qa.positive_doc_ids)
    valid_ids = list(needed_ids.intersection(available_pdfs_ids))

    if LIMIT_PDFS > 0 and len(valid_ids) > LIMIT_PDFS:
        target_doc_ids = set(random.sample(valid_ids, LIMIT_PDFS))
    else:
        target_doc_ids = set(valid_ids)

    print(f"ğŸ“Š í‰ê°€ ê·œëª¨: {len(target_doc_ids)}ê°œ ë…¼ë¬¸")

    # 2. ë¬¸ì„œ ë¡œë“œ ë° ë…¸ë“œ ìƒì„±
    loader = OptimizedPDFLoader(PDF_DIR)
    docs = loader.load_specific_documents(target_doc_ids)
    
    print("ğŸ”¨ ë¬¸ì„œë¥¼ ë…¸ë“œë¡œ ë¶„í•  ì¤‘...")
    nodes = parser.get_nodes_from_documents(docs)

    # 3. Property Graph Index ìƒì„± (Fast Mode)
    print(f"\nğŸ—ï¸  Property Graph Index ìƒì„± ì¤‘ (Nodes: {len(nodes)})...")
    
    # [ìˆ˜ì •ë¨] LLM ëŒ€ì‹  ImplicitPathExtractor ì‚¬ìš©
    # ë¬¸ì„œì˜ ìˆœì„œ(Next/Prev)ì™€ ì†Œì†(Parent) ê´€ê³„ë§Œìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ë§Œë“­ë‹ˆë‹¤. (ë§¤ìš° ë¹ ë¦„)
    index = PropertyGraphIndex(
        nodes=nodes,
        kg_extractors=[ImplicitPathExtractor()], 
        embed_model=Settings.embed_model,
        llm=Settings.llm,
        show_progress=True
    )
    
    # 4. Hybrid Retriever êµ¬ì„±
    print("ğŸ”— Hybrid Retriever êµ¬ì„±...")
    
    # (A) Graph Retriever
    pg_retriever = index.as_retriever(
        include_text=True, 
        similarity_top_k=20
    )
    
    # (B) BM25 Retriever
    bm25_retriever = BM25Retriever.from_defaults(
        nodes=nodes,
        similarity_top_k=20,
        language="english"
    )
    
    # (C) Fusion
    hybrid_retriever = QueryFusionRetriever(
        [pg_retriever, bm25_retriever],
        num_queries=1,
        mode=FUSION_MODES.RECIPROCAL_RANK,
        use_async=True,
        similarity_top_k=20
    )

    # 5. Re-ranking
    print("ğŸ¯ Re-ranker ë¡œë”© ì¤‘...")
    reranker = SentenceTransformerRerank(
        model="cross-encoder/ms-marco-MiniLM-L-6-v2", 
        top_n=max(TOP_K_LIST) 
    )

    # 6. í‰ê°€ ì§„í–‰
    eval_qa_list = [qa for qa in full_qa_list if not qa.positive_doc_ids.isdisjoint(target_doc_ids)]
    print(f"\nğŸ” í‰ê°€ ì‹œì‘ (ì´ {len(eval_qa_list)}ê°œ ì§ˆë¬¸)")
    
    metrics = {f"recall@{k}": 0.0 for k in TOP_K_LIST}
    metrics.update({f"mrr@{k}": 0.0 for k in TOP_K_LIST})

    for i, ex in enumerate(tqdm(eval_qa_list, desc="Evaluating")):
        try:
            initial_nodes = hybrid_retriever.retrieve(ex.question)
            
            reranked_nodes = reranker.postprocess_nodes(
                initial_nodes, 
                query_str=ex.question
            )
            final_nodes = reranked_nodes
            
        except Exception as e:
            # print(f"Error: {e}")
            final_nodes = []
        
        retrieved_doc_ids = [node.metadata.get("doc_id", "") for node in final_nodes]
        gt_set = ex.positive_doc_ids

        if i < 3:
            tqdm.write(f"\n[Q] {ex.question}")
            tqdm.write(f"   Target: {list(gt_set)}")
            tqdm.write(f"   Pred:   {retrieved_doc_ids[:5]}")
            hit = any(d in gt_set for d in retrieved_doc_ids[:5])
            tqdm.write(f"   -> {'âœ… HIT' if hit else 'âŒ MISS'}")

        for k in TOP_K_LIST:
            current_top_k = retrieved_doc_ids[:min(k, len(retrieved_doc_ids))]
            if any(did in gt_set for did in current_top_k):
                metrics[f"recall@{k}"] += 1.0
            for rank, did in enumerate(current_top_k, start=1):
                if did in gt_set:
                    metrics[f"mrr@{k}"] += 1.0 / rank
                    break

    # ìµœì¢… ê²°ê³¼
    count = len(eval_qa_list)
    print("\n" + "="*50)
    print(f"ğŸ“ˆ ìµœì¢… Fast Graph í‰ê°€ ê²°ê³¼ (Samples: {count})")
    print("="*50)
    
    if count > 0:
        for k in TOP_K_LIST:
            recall = metrics[f'recall@{k}'] / count
            mrr = metrics[f'mrr@{k}'] / count
            print(f"Recall@{k:<2}   | {recall:.4f}")
            print(f"MRR@{k:<2}      | {mrr:.4f}")

if __name__ == "__main__":
    evaluate_system()